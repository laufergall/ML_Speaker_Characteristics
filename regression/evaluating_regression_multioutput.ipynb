{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating regression techniques for speaker characterization\n",
    "### Laura Fernández Gallardo\n",
    "\n",
    "Multioutput regression: for each instance, targeting the prediction of the 5 dimensions of perceptive speaker characteristics.\n",
    "\n",
    "Motifications with respect to regression with 1-dimensional output:\n",
    "\n",
    "* targets: 5-dimensional scores derived from factor analysis on the 34-dimensional ratings of speaker characteristics in the [subjective analysis](https://github.com/laufergall/Subjective_Speaker_Characteristics).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from reg_tuning import * # my helper functions\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 2302\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features and ratings from the regression task with 1d output\n",
    "\n",
    "feats_ratings_train = pd.read_csv(r'.\\data_while_tuning\\feats_ratings_train.csv')\n",
    "feats_ratings_test = pd.read_csv(r'.\\data_while_tuning\\feats_ratings_test.csv')\n",
    "\n",
    "sc_names = ['non_likable', 'secure', 'attractive', 'unsympathetic', 'indecisive', 'unobtrusive', 'distant', 'bored', 'emotional', 'not_irritated', 'active', 'pleasant', 'characterless', 'sociable', 'relaxed', 'affectionate', 'dominant', 'unaffected', 'hearty', 'old', 'personal', 'calm', 'incompetent', 'ugly', 'friendly', 'masculine', 'submissive', 'indifferent', 'interesting', 'cynical', 'artificial', 'intelligent', 'childish', 'modest']\n",
    "\n",
    "dropcolumns = ['name','spkID','speaker_gender'] + sc_names\n",
    "feats_names = list(feats_ratings_train.drop(dropcolumns, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attractiveness</th>\n",
       "      <th>compliance</th>\n",
       "      <th>confidence</th>\n",
       "      <th>maturity</th>\n",
       "      <th>sample_heard</th>\n",
       "      <th>warmth</th>\n",
       "      <th>gender</th>\n",
       "      <th>spkID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.579301</td>\n",
       "      <td>-0.921918</td>\n",
       "      <td>0.608503</td>\n",
       "      <td>0.276580</td>\n",
       "      <td>m004_linden_stimulus.wav</td>\n",
       "      <td>-0.284638</td>\n",
       "      <td>m</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.442865</td>\n",
       "      <td>-0.950212</td>\n",
       "      <td>0.588889</td>\n",
       "      <td>0.630295</td>\n",
       "      <td>m005_nicosia_stimulus.wav</td>\n",
       "      <td>-0.494019</td>\n",
       "      <td>m</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.507534</td>\n",
       "      <td>0.139302</td>\n",
       "      <td>-0.151077</td>\n",
       "      <td>-0.669449</td>\n",
       "      <td>m006_rabat_stimulus.wav</td>\n",
       "      <td>1.533478</td>\n",
       "      <td>m</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.180748</td>\n",
       "      <td>-0.108982</td>\n",
       "      <td>0.962166</td>\n",
       "      <td>1.026359</td>\n",
       "      <td>m007_klaksvik_stimulus.wav</td>\n",
       "      <td>0.478983</td>\n",
       "      <td>m</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.070247</td>\n",
       "      <td>-0.284278</td>\n",
       "      <td>-0.875589</td>\n",
       "      <td>-1.291311</td>\n",
       "      <td>m016_beirut_stimulus.wav</td>\n",
       "      <td>1.861551</td>\n",
       "      <td>m</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   attractiveness  compliance  confidence  maturity  \\\n",
       "0       -0.579301   -0.921918    0.608503  0.276580   \n",
       "1        0.442865   -0.950212    0.588889  0.630295   \n",
       "2       -0.507534    0.139302   -0.151077 -0.669449   \n",
       "3        1.180748   -0.108982    0.962166  1.026359   \n",
       "4        1.070247   -0.284278   -0.875589 -1.291311   \n",
       "\n",
       "                 sample_heard    warmth gender  spkID  \n",
       "0    m004_linden_stimulus.wav -0.284638      m      4  \n",
       "1   m005_nicosia_stimulus.wav -0.494019      m      5  \n",
       "2     m006_rabat_stimulus.wav  1.533478      m      6  \n",
       "3  m007_klaksvik_stimulus.wav  0.478983      m      7  \n",
       "4    m016_beirut_stimulus.wav  1.861551      m     16  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# speaker scores\n",
    "\n",
    "path = \"https://raw.githubusercontent.com/laufergall/Subjective_Speaker_Characteristics/master/data/generated_data/\"\n",
    "\n",
    "url = path + \"factorscores_malespk.csv\"\n",
    "s = requests.get(url).content\n",
    "scores_m =pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "\n",
    "url = path + \"factorscores_femalespk.csv\"\n",
    "s = requests.get(url).content\n",
    "scores_f =pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "\n",
    "# rename dimensions\n",
    "scores_m.columns = ['sample_heard', 'warmth', 'attractiveness', 'confidence', 'compliance', 'maturity']\n",
    "scores_f.columns = ['sample_heard', 'warmth', 'attractiveness', 'compliance', 'confidence', 'maturity']\n",
    "\n",
    "# join male and feame scores\n",
    "scores = scores_m.append(scores_f)\n",
    "scores['gender'] = scores['sample_heard'].str.slice(0,1)\n",
    "scores['spkID'] = scores['sample_heard'].str.slice(1,4).astype('int')\n",
    "\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge scores and features\n",
    "\n",
    "feats_ratings_scores_train = feats_ratings_train.merge(scores) # (2700, 132)\n",
    "feats_ratings_scores_test = feats_ratings_test.merge(scores) # (891, 132)\n",
    "\n",
    "# drop unnecessary columns\n",
    "feats_ratings_scores_train = feats_ratings_scores_train.drop(['speaker_gender','sample_heard'] + sc_names, axis = 1)\n",
    "feats_ratings_scores_test = feats_ratings_scores_test.drop(['speaker_gender','sample_heard'] + sc_names, axis = 1)\n",
    "\n",
    "# 'name' + 88 features + 5 traits + 'gender' + 'spkID'\n",
    "# shape train: (2700, 96), shape test: (891, 96) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize speech features  \n",
    "\n",
    "dropcolumns = ['name','gender','spkID'] + list(scores_m.columns)[1:]\n",
    "\n",
    "# learn transformation on training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(feats_ratings_scores_train.drop(dropcolumns, axis=1))\n",
    "\n",
    "# numpy n_instances x n_feats\n",
    "feats_s_train = scaler.transform(feats_ratings_scores_train.drop(dropcolumns, axis=1))\n",
    "feats_s_test = scaler.transform(feats_ratings_scores_test.drop(dropcolumns, axis=1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tuning with feature selection\n",
    "\n",
    "Use the train data to find the classifier and its hyperparameters leading to the best performance. \n",
    "\n",
    "Not performing feature selection with \"SelectKBest\": Univariate feature selection does not support multilabel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances in A (hyperparameter tuning): 2160\n",
      "Number of instances in B (meta-evaluation): 540\n"
     ]
    }
   ],
   "source": [
    "# training data. Features and labels\n",
    "X = feats_s_train # (2700, 88)\n",
    "y = feats_ratings_scores_train[['warmth','attractiveness']].as_matrix() # (2700, 2)\n",
    "\n",
    "# test data. Features and labels\n",
    "Xt = feats_s_test # (891, 88)\n",
    "yt = feats_ratings_scores_test[['warmth','attractiveness']].as_matrix() # (891, 2)\n",
    "\n",
    "# split train data into 80% and 20% subsets - with balance in trait and gender\n",
    "# give subset A to the inner hyperparameter tuner\n",
    "# and hold out subset B for meta-evaluation\n",
    "AX, BX, Ay, By = train_test_split(X, y, test_size=0.20, stratify = feats_ratings_scores_train['gender'], random_state=2302)\n",
    "\n",
    "print('Number of instances in A (hyperparameter tuning):',AX.shape[0])\n",
    "print('Number of instances in B (meta-evaluation):',BX.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataframe with results from hp tuner to be appended\n",
    "tuning_all = pd.DataFrame()\n",
    "\n",
    "# list with tuned classifiers trained on training data, to be appended\n",
    "trained_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save splits\n",
    "\n",
    "# original features and class\n",
    "feats_ratings_scores_train.to_csv(r'.\\data_while_tuning\\feats_ratings_scores_train.csv', index=False)\n",
    "feats_ratings_scores_test.to_csv(r'.\\data_while_tuning\\feats_ratings_scores_test.csv', index=False)\n",
    "\n",
    "# train/test partitions, features and labels\n",
    "np.save(r'.\\data_while_tuning\\X_multioutput_WAAT.npy', X)\n",
    "np.save(r'.\\data_while_tuning\\y_multioutput_WAAT.npy', y)\n",
    "np.save(r'.\\data_while_tuning\\Xt_multioutput_WAAT.npy', Xt)\n",
    "np.save(r'.\\data_while_tuning\\yt_multioutput_WAAT.npy', yt)\n",
    "\n",
    "# # A/B splits, features and labels\n",
    "np.save(r'.\\data_while_tuning\\AX_multioutput_WAAT.npy', AX)\n",
    "np.save(r'.\\data_while_tuning\\BX_multioutput_WAAT.npy', BX)\n",
    "np.save(r'.\\data_while_tuning\\Ay_multioutput_WAAT.npy', Ay)\n",
    "np.save(r'.\\data_while_tuning\\By_multioutput_WAAT.npy', By)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling hp_tuner() for each classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Recover ** when new ipynb session started.\n",
    "\n",
    "(Workaround for working with hyperparameter tuning during several days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# original features and class\n",
    "feats_ratings_scores_train = pd.read_csv(r'.\\data_while_tuning\\feats_ratings_scores_train.csv')\n",
    "feats_ratings_scores_test = pd.read_csv(r'.\\data_while_tuning\\feats_ratings_scores_test.csv')\n",
    "feats_names = pd.read_csv(r'.\\data_while_tuning\\feats_names.csv', header=None)\n",
    "feats_names = feats_names.values.tolist()\n",
    "\n",
    "# train/test partitions, features and labels\n",
    "X = np.load(r'.\\data_while_tuning\\X_multioutput_WAAT.npy')\n",
    "y = np.load(r'.\\data_while_tuning\\y_multioutput_WAAT.npy')\n",
    "Xt = np.load(r'.\\data_while_tuning\\Xt_multioutput_WAAT.npy')\n",
    "yt = np.load(r'.\\data_while_tuning\\yt_multioutput_WAAT.npy')\n",
    "\n",
    "# A/B splits, features and labels\n",
    "AX = np.load(r'.\\data_while_tuning\\AX_multioutput_WAAT.npy')\n",
    "BX = np.load(r'.\\data_while_tuning\\BX_multioutput_WAAT.npy')\n",
    "Ay = np.load(r'.\\data_while_tuning\\Ay_multioutput_WAAT.npy')\n",
    "By = np.load(r'.\\data_while_tuning\\By_multioutput_WAAT.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading outpus of hp tuning from disk\n",
    "tuning_all, trained_all = load_tuning('multioutput_WAAT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call this after each experiment **to recover later**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save tuning_all (.csv) and trained_all (nameclassifier.sav)\n",
    "save_tuning(tuning_all, trained_all, target_trait,'multioutput_WAAT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree\n",
    "\n",
    "*class sklearn.tree.DecisionTreeRegressor(criterion=’mse’, splitter=’best’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, presort=False)*\n",
    "\n",
    "Tune: max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'DecisionTreeRegressor' -> Best cross-val score on A set: -2.060822 using {'regressor__estimator__max_depth': 3}\n",
      "'DecisionTreeRegressor' -> root mean_squared_error on B set: 1.442424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\"\"\"\n",
    "Decision Trees\n",
    "\"\"\"\n",
    "def get_DecisionTreeRegressor2tune():\n",
    "    \n",
    "    model = DecisionTreeRegressor()\n",
    "    hp = dict(\n",
    "        regressor__estimator__max_depth = np.arange(2,20) \n",
    "    )\n",
    "    return 'DecisionTreeRegressor', model, hp\n",
    "\n",
    "\n",
    "# tune this model (multiputput)\n",
    "tuning, trained = hp_tuner(AX, BX, Ay, By, \n",
    "                           [get_DecisionTreeRegressor2tune], \n",
    "                           'multioutput_WAAT',\n",
    "                           feats_names,\n",
    "                           [88], # feature selection not performed\n",
    "                           mode='random',\n",
    "                           n_iter=10\n",
    "                          )\n",
    "\n",
    "# update lists of tuning info and trained classifiers\n",
    "tuning_all = tuning_all.append(tuning, ignore_index=True)\n",
    "trained_all.append(trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "\n",
    "*class sklearn.ensemble.RandomForestRegressor(n_estimators=10, criterion=’mse’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False)*\n",
    "\n",
    "Tune: max_features, max_depth, min_samples_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\"\"\"\n",
    "Random Forest\n",
    "\"\"\"\n",
    "def get_RandomForestRegressor2tune():\n",
    "    \n",
    "    model = DecisionTreeRegressor()\n",
    "    hp = dict(\n",
    "        regressor__estimator__max_features = np.arange(2,50),\n",
    "        regressor__estimator__max_depth = np.arange(2,50), \n",
    "        regressor__estimator__min_samples_leaf = np.arange(2,100) \n",
    "    )\n",
    "    return 'RandomForestRegressor', model, hp\n",
    "\n",
    "\n",
    "# tune this model (multiputput)\n",
    "tuning, trained = hp_tuner(AX, BX, Ay, By, \n",
    "                           [get_RandomForestRegressor2tune], \n",
    "                           'multioutput_WAAT',\n",
    "                           feats_names,\n",
    "                           [88], # feature selection not performed\n",
    "                           mode='random',\n",
    "                           n_iter=10\n",
    "                          )\n",
    "\n",
    "# update lists of tuning info and trained classifiers\n",
    "tuning_all = tuning_all.append(tuning, ignore_index=True)\n",
    "trained_all.append(trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "### RMSE\n",
    "Testing the best model found after hyperparameter tuning.\n",
    "\n",
    "Performance metric: RMSE (see above).\n",
    "    \n",
    "The predictions correspond to the scores of each test segment (3 parts x 4 dialogs) spoken by the same test speaker. I perform the average of the predicted scores that correspond to the same speaker - to be compared to the true scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the classifier that gave the maximum acc on B set\n",
    "best_accs = tuning_all['best_accs']\n",
    "i_best = best_accs.idxmin()\n",
    "\n",
    "print('Selected classifier based on the best performance on B: %r (accB = %0.2f)' % (tuning_all.loc[i_best,'regressors_names'], round(best_accs[i_best],2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute predictions with the best tuned regressor\n",
    "\n",
    "yt_pred = trained_all[i_best][0].predict(Xt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# average of outputs that belong to the same speaker\n",
    "\n",
    "test_scores = pd.DataFrame(data = feats_ratings_scores_test[['warmth','attractiveness','spkID']])\n",
    "test_scores['warmth_pred'] = yt_pred[:, 0]\n",
    "test_scores['attractiveness_pred'] = yt_pred[:, 1]\n",
    "\n",
    "test_scores_avg = test_scores.groupby('spkID').mean()\n",
    "\n",
    "myrmse_avg = np.sqrt(mean_squared_error(test_scores_avg[['warmth','attractiveness']].as_matrix(), \n",
    "             test_scores_avg[['warmth_pred','attractiveness_pred']].as_matrix()))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "1) WAAT scores of test data and averaged predicted test data.\n",
    "\n",
    "2) animation (.html) of pairs of true-predicted WAAT scores. For each frame i, 2 points are plotted in 2D: one corresponding to the true WAAT of speaker i and the other one corresponding to the averaged predicted WAAT of the same speaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(['default'])\n",
    "alpha = 0.5\n",
    "\n",
    "plt.figure()\n",
    "# plt.scatter(y[:, 0], y[:, 1],\n",
    "#             marker=\"o\", alpha=alpha, label=\"Train data\")\n",
    "plt.scatter(test_scores_avg['warmth'], test_scores_avg['attractiveness'],\n",
    "            marker=\"o\", alpha=alpha, label=\"Test data\")\n",
    "plt.scatter(test_scores_avg['warmth_pred'], test_scores_avg['attractiveness_pred'],\n",
    "            marker=\"^\", alpha=alpha,\n",
    "            label=\"RMSE=%.2f\" % myrmse_avg)\n",
    "\n",
    "plt.xlabel('warmth')\n",
    "plt.ylabel('attractiveness')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# save figure\n",
    "filename = r'\\multioutput_test_'+tuning_all.loc[i_best,'regressors_names']+'.png'\n",
    "plt.savefig(r'.\\figures' + filename, bbox_inches = 'tight')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores_avg.reset_index(inplace=True)\n",
    "true_scores = test_scores_avg[['spkID','warmth','attractiveness']]\n",
    "true_scores['type']='true'\n",
    "pred_scores = test_scores_avg[['spkID','warmth_pred','attractiveness_pred']].rename(index=str, \n",
    "                                                                                    columns={\"warmth_pred\": \"warmth\", \n",
    "                                                                                             \"attractiveness_pred\": \"attractiveness\"})\n",
    "pred_scores['type']='pred'\n",
    "test_scores_avg_WAAT = true_scores.append(pred_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "unique_speakers = test_scores_avg_WAAT['spkID'].unique()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot a scatter that persists(isn't redrawn) \n",
    "ax.set_xlabel('warmth')\n",
    "ax.set_ylabel('attractiveness')\n",
    "ax.set_xlim(-6, 4)  \n",
    "ax.set_ylim(-3, 2)        \n",
    "\n",
    "def update(i):\n",
    "    spk=unique_speakers[i]\n",
    "    coor_x = test_scores_avg_WAAT.loc[test_scores_avg_WAAT['spkID']==spk,'warmth']\n",
    "    coor_y = test_scores_avg_WAAT.loc[test_scores_avg_WAAT['spkID']==spk,'attractiveness']\n",
    "    ax.scatter(coor_x, coor_y)\n",
    "    ax.plot(coor_x, coor_y) \n",
    "    return ax\n",
    "\n",
    "# save animation\n",
    "anim = FuncAnimation(fig, update, frames=np.arange(0, len(unique_speakers)), interval=200)\n",
    "filename = r'\\multioutput_test_'+tuning_all.loc[i_best,'regressors_names']+'.html'\n",
    "anim.save(r'.\\figures' + filename, dpi=80, writer='imagemagick')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "* tune to reduce RMSE\n",
    "* output RMSE for warmth and for attractiveness separately\n",
    "\n",
    "* same for the 5 traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
