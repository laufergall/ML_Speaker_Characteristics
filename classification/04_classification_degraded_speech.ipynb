{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating classification techniques for speaker characterization\n",
    "### Laura Fernández Gallardo\n",
    "\n",
    "Using the models trained with clean speech, analyze the effects of testing with distorted speech on the classification performance.\n",
    "\n",
    "* WAAT classification\n",
    "* multilabel classification\n",
    "\n",
    "As done when tuning models, the evaluation metric considered is the **average per-class accuracy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "import pickle # save / load models\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 2302\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech degradations\n",
    "\n",
    "The speech degradations employed in this study are outline in the [README.md file](https://github.com/laufergall/ML_Speaker_Characteristics) of this repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load features from clean speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load features from degraded speech\n",
    "\n",
    "# (gitignored file)\n",
    "file_feats = r'..\\data\\extracted_features\\eGeMAPSv01a_semispontaneous_splitted_distorted.csv'\n",
    "feats_dist = pd.read_csv(file_feats) # shape feats_dist\n",
    "\n",
    "# distortion code\n",
    "feats_dist['distnum'] = feats_dist['name'].str.slice(5,7)\n",
    "\n",
    "# add column with packet loss info\n",
    "feats_dist['packetlossrate'] = feats_dist['name'].str.extract('(?<=P)(.*?)(?=-)', expand=False)\n",
    "\n",
    "## add column with jitter info\n",
    "feats_dist['jitterms'] = feats_dist['name'].str.extract('(?<=J)(.*?)(?=%)', expand=False)\n",
    "\n",
    "# load mapping of degradation names\n",
    "path = \"https://raw.githubusercontent.com/laufergall/ML_Speaker_Characteristics/master/data/distortions/\"\n",
    "url = path + \"dist_mapping.csv\"\n",
    "s = requests.get(url).content\n",
    "dist_mapping =pd.read_csv(io.StringIO(s.decode('utf-8')), sep = ';')\n",
    "\n",
    "# padding distortion codes with zeroes\n",
    "dist_mapping['distnum'] = dist_mapping['distnum'].astype(str).str.zfill(2)\n",
    "\n",
    "# merge feats with distmapping to know name of distortions\n",
    "feats_dist = feats_dist.merge(dist_mapping)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SWB_Opus_24       4493\n",
      "SWB_EVS_7_2       4482\n",
      "WB_Speex_23_8     4478\n",
      "SWB_G7221C_48     4477\n",
      "WB_AMRWB+_24      4477\n",
      "NB_GSMEFR_12_2    4476\n",
      "NB_Speex_2_15     4471\n",
      "SWB_EVS_128       4469\n",
      "WB_AMRWB+_13_6    4466\n",
      "WB_Speex_42_2     4465\n",
      "WB_AMRWB_12_65    4465\n",
      "SWB_Opus_32       4464\n",
      "SWB_Opus_64       4464\n",
      "NB_AMRNB_6_7      4460\n",
      "WB_AMRWB_23_85    4460\n",
      "WB_Speex_3_95     4459\n",
      "SWB_EVS_24_4      4459\n",
      "WB_AMRWB_23_05    4458\n",
      "WB_AMRWB+_20_8    4456\n",
      "NB_AMRNB_7_4      4456\n",
      "NB_G7231_6_3      4455\n",
      "SWB_EVS_48        4454\n",
      "NB_G711_A_64      4454\n",
      "NB_AMRNB_5_9      4454\n",
      "NB_G7231_5_3      4453\n",
      "WB_G722_64        4452\n",
      "NB_AMRNB_4_75     4451\n",
      "NB_AMRNB_10_2     4451\n",
      "SWB_EVS_96        4451\n",
      "WB_AMRWB_8_85     4448\n",
      "WB_AMRWB_19_85    4446\n",
      "WB_AMRWB+_15_2    4445\n",
      "NB_AMRNB_5_15     4445\n",
      "WB_AMRWB_14_25    4444\n",
      "SWB_G7221C_24     4443\n",
      "WB_AMRWB+_16_8    4441\n",
      "SWB_Opus_160      4440\n",
      "NB_Speex_11       4439\n",
      "SWB_EVS_64        4439\n",
      "WB_AMRWB+_10_4    4437\n",
      "WB_AMRWB+_12      4437\n",
      "NB_G711_u_64      4432\n",
      "NB_AMRNB_7_95     4429\n",
      "SWB_EVS_32        4423\n",
      "WB_AMRWB_15_85    4421\n",
      "NB_Speex_24_6     4420\n",
      "SWB_G7221C_32     4419\n",
      "NB_AMRNB_12_2     4416\n",
      "WB_AMRWB+_19_2    4411\n",
      "SWB_Opus_48       4409\n",
      "WB_AMRWB_18_25    4402\n",
      "SWB_Opus_128      4400\n",
      "WB_AMRWB_6_6      4393\n",
      "Name: distcode, dtype: int64\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "# how many degradations\n",
    "print(feats_dist['distcode'].value_counts()) # around 4490 instances for each degradation\n",
    "print(len(feats_dist['distcode'].value_counts())) # 53 degradations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distnum</th>\n",
       "      <th>distcode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00</td>\n",
       "      <td>Clean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01</td>\n",
       "      <td>NB_G711_A_64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02</td>\n",
       "      <td>NB_G711_u_64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>03</td>\n",
       "      <td>NB_G7231_5_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04</td>\n",
       "      <td>NB_G7231_6_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>05</td>\n",
       "      <td>NB_GSMEFR_12_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>06</td>\n",
       "      <td>NB_AMRNB_4_75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>07</td>\n",
       "      <td>NB_AMRNB_5_15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>08</td>\n",
       "      <td>NB_AMRNB_5_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>09</td>\n",
       "      <td>NB_AMRNB_6_7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>NB_AMRNB_7_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>NB_AMRNB_7_95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>NB_AMRNB_10_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>NB_AMRNB_12_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>NB_Speex_2_15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>NB_Speex_11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>NB_Speex_24_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>WB_G722_64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>WB_AMRWB_6_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>WB_AMRWB_8_85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>WB_AMRWB_12_65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>WB_AMRWB_14_25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>WB_AMRWB_15_85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>WB_AMRWB_18_25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>WB_AMRWB_19_85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>WB_AMRWB_23_05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>WB_AMRWB_23_85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>WB_Speex_3_95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>WB_Speex_23_8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>WB_Speex_42_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>WB_AMRWB+_10_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>WB_AMRWB+_12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>WB_AMRWB+_13_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>WB_AMRWB+_15_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>WB_AMRWB+_16_8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>WB_AMRWB+_19_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>WB_AMRWB+_20_8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>WB_AMRWB+_24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>SWB_G7221C_24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>SWB_G7221C_32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>SWB_G7221C_48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>SWB_EVS_7_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>SWB_EVS_24_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>SWB_EVS_32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>SWB_EVS_48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>SWB_EVS_64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>SWB_EVS_96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>SWB_EVS_128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>SWB_Opus_24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>SWB_Opus_32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>SWB_Opus_48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>51</td>\n",
       "      <td>SWB_Opus_64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>52</td>\n",
       "      <td>SWB_Opus_128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>53</td>\n",
       "      <td>SWB_Opus_160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   distnum        distcode\n",
       "0       00           Clean\n",
       "1       01    NB_G711_A_64\n",
       "2       02    NB_G711_u_64\n",
       "3       03    NB_G7231_5_3\n",
       "4       04    NB_G7231_6_3\n",
       "5       05  NB_GSMEFR_12_2\n",
       "6       06   NB_AMRNB_4_75\n",
       "7       07   NB_AMRNB_5_15\n",
       "8       08    NB_AMRNB_5_9\n",
       "9       09    NB_AMRNB_6_7\n",
       "10      10    NB_AMRNB_7_4\n",
       "11      11   NB_AMRNB_7_95\n",
       "12      12   NB_AMRNB_10_2\n",
       "13      13   NB_AMRNB_12_2\n",
       "14      14   NB_Speex_2_15\n",
       "15      15     NB_Speex_11\n",
       "16      16   NB_Speex_24_6\n",
       "17      17      WB_G722_64\n",
       "18      18    WB_AMRWB_6_6\n",
       "19      19   WB_AMRWB_8_85\n",
       "20      20  WB_AMRWB_12_65\n",
       "21      21  WB_AMRWB_14_25\n",
       "22      22  WB_AMRWB_15_85\n",
       "23      23  WB_AMRWB_18_25\n",
       "24      24  WB_AMRWB_19_85\n",
       "25      25  WB_AMRWB_23_05\n",
       "26      26  WB_AMRWB_23_85\n",
       "27      27   WB_Speex_3_95\n",
       "28      28   WB_Speex_23_8\n",
       "29      29   WB_Speex_42_2\n",
       "30      30  WB_AMRWB+_10_4\n",
       "31      31    WB_AMRWB+_12\n",
       "32      32  WB_AMRWB+_13_6\n",
       "33      33  WB_AMRWB+_15_2\n",
       "34      34  WB_AMRWB+_16_8\n",
       "35      35  WB_AMRWB+_19_2\n",
       "36      36  WB_AMRWB+_20_8\n",
       "37      37    WB_AMRWB+_24\n",
       "38      38   SWB_G7221C_24\n",
       "39      39   SWB_G7221C_32\n",
       "40      40   SWB_G7221C_48\n",
       "41      41     SWB_EVS_7_2\n",
       "42      42    SWB_EVS_24_4\n",
       "43      43      SWB_EVS_32\n",
       "44      44      SWB_EVS_48\n",
       "45      45      SWB_EVS_64\n",
       "46      46      SWB_EVS_96\n",
       "47      47     SWB_EVS_128\n",
       "48      48     SWB_Opus_24\n",
       "49      49     SWB_Opus_32\n",
       "50      50     SWB_Opus_48\n",
       "51      51     SWB_Opus_64\n",
       "52      52    SWB_Opus_128\n",
       "53      53    SWB_Opus_160"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WAAT classification\n",
    "\n",
    "The WAAT (warmth-attractiveness) score distribution was already explored in Part I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load WAAT scores (averaged across listeners)\n",
    "\n",
    "path = \"https://raw.githubusercontent.com/laufergall/Subjective_Speaker_Characteristics/master/data/generated_data/\"\n",
    "\n",
    "url = path + \"factorscores_malespk.csv\"\n",
    "s = requests.get(url).content\n",
    "scores_m =pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "\n",
    "url = path + \"factorscores_femalespk.csv\"\n",
    "s = requests.get(url).content\n",
    "scores_f =pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "\n",
    "# rename dimensions\n",
    "scores_m.columns = ['sample_heard', 'warmth', 'attractiveness', 'confidence', 'compliance', 'maturity']\n",
    "scores_f.columns = ['sample_heard', 'warmth', 'attractiveness', 'compliance', 'confidence', 'maturity']\n",
    "\n",
    "# join male and feame scores\n",
    "scores = scores_m.append(scores_f)\n",
    "scores['gender'] = scores['sample_heard'].str.slice(0,1)\n",
    "scores['spkID'] = scores['sample_heard'].str.slice(1,4).astype('int')\n",
    "\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# scatter plot\n",
    "\n",
    "sns.lmplot('warmth', 'attractiveness', data = scores, hue=\"gender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram, kernel density estimation\n",
    "sns.jointplot('warmth', 'attractiveness', data = scores, kind=\"kde\").set_axis_labels(\"warmth\", \"attractiveness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get 3 clusters of speakers based on the WAAT distribution.\n",
    "Each cluster with approx. the same number of instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# applying k-means\n",
    "\n",
    "n_clusters=3\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=2302).fit(scores[['warmth','attractiveness']])\n",
    "\n",
    "scores['class'] = pd.Categorical(kmeans.labels_).rename_categories(['low','high','mid'])\n",
    "\n",
    "sns.lmplot('warmth', 'attractiveness', data = scores, hue=\"class\",fit_reg=False)\n",
    " \n",
    "print(scores['class'].value_counts())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select trait for binary classification and perform data partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing** speakers in the mid class to address binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# remove speakers in the mid class\n",
    "\n",
    "scores = scores.loc[ scores['class'] != 'mid', ['spkID','gender','class']]\n",
    "\n",
    "scores.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores['class'] = pd.Categorical(scores['class'], categories=['low','high'])\n",
    "\n",
    "print(scores.groupby(['gender','class']).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split speakers into train (75%) and test (25%) speakers with class and gender balance (stratified) by creating the dummy \"gendertrait\" class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get stratified random partition for train and test\n",
    "\n",
    "scores['genderclass']=scores[['gender', 'class']].apply(lambda x: ''.join(x), axis=1)\n",
    "\n",
    "indexes = np.arange(0,len(scores))\n",
    "classes = scores['class']\n",
    "train_i, test_i, train_y, test_y = train_test_split(indexes, \n",
    "                                                    classes, \n",
    "                                                    test_size=0.25, \n",
    "                                                    stratify = scores['genderclass'], \n",
    "                                                    random_state=2302)\n",
    "\n",
    "scores_train = scores.iloc[train_i,:] \n",
    "scores_test = scores.iloc[test_i,:] \n",
    "\n",
    "print('Number of speakers in Train:',len(scores_train))\n",
    "print('Number of speakers in Test:',len(scores_test))\n",
    "\n",
    "print('Number of w-high speakers in Train:', len(scores_train.loc[scores_train['genderclass']=='whigh']) )\n",
    "print('Number of m-high speakers in Train:', len(scores_train.loc[scores_train['genderclass']=='mhigh']) )\n",
    "print('Number of w-low speakers in Train:', len(scores_train.loc[scores_train['genderclass']=='wlow']) )\n",
    "print('Number of m-high speakers in Train:', len(scores_train.loc[scores_train['genderclass']=='mlow']) )\n",
    "\n",
    "print('Number of w-high speakers in Test:', len(scores_test.loc[scores_test['genderclass']=='whigh']) )\n",
    "print('Number of m-high speakers in Test:', len(scores_test.loc[scores_test['genderclass']=='mhigh']) )\n",
    "print('Number of w-low speakers in Test:', len(scores_test.loc[scores_test['genderclass']=='wlow']) )\n",
    "print('Number of m-low speakers in Test:', len(scores_test.loc[scores_test['genderclass']=='mlow']) )\n",
    "\n",
    "\n",
    "# # save these data for other evaluations\n",
    "scores_train.iloc[:,0:3].to_csv(r'..\\data\\generated_data\\speakerIDs_cls_WAAT_train.csv', index=False)\n",
    "scores_test.iloc[:,0:3].to_csv(r'..\\data\\generated_data\\speakerIDs_clss_WAAT_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech features\n",
    "\n",
    "Speech features have been extracted from the semi-spontaneous dialogs uttered by the 300 speakers of the [NSC corpus](http://www.qu.tu-berlin.de/?id=nsc-corpus). \n",
    "\n",
    "Each semi-spontaneous dialog was splitted into 3 segments of approx. 20s, and the 88 [eGeMAPS](http://ieeexplore.ieee.org/document/7160715/) speech features were extracted from each segment (see ..\\feature_extraction).\n",
    "\n",
    "299 speakers recorded 4 semi-spontaneous dialogs, and 1 female speaker recorded 1 semi-spontaneous dialog. Total = 1197 dialogs * 3 segments = 3591 speech files.\n",
    "\n",
    "Unfortunately, no subjective ratings have been collected for the spontaneous dialogs d5, d7, or d8. However, we use the speech features in order to have more instances with which to train and test the models.\n",
    "\n",
    "**I assume** that the speakers' trait classes remain constant across recordings, that is, is a speaker is perceived as 'high' in the _intelligent_ trait for dialog 6 (d6, pizza dialog), then this perception would be the same for the other dialogs uttered by the same speaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load speech features\n",
    "\n",
    "path = \"https://raw.githubusercontent.com/laufergall/ML_Speaker_Characteristics/master/data/extracted_features/\"\n",
    "\n",
    "url = path + \"/eGeMAPSv01a_semispontaneous_splitted.csv\"\n",
    "s = requests.get(url).content\n",
    "feats =pd.read_csv(io.StringIO(s.decode('utf-8')), sep = ';') # shape: 3591, 89\n",
    "\n",
    "feats.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Pre-processing features with the transformation **learnt with training data**:\n",
    "\n",
    "* center and scale speech features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Separate instances according to the train and test partition\n",
    "# instances corresponding to speakers in the mid class will be left out\n",
    "\n",
    "# extract speaker ID from speech file name\n",
    "feats['spkID'] = feats['name'].str.slice(2, 5).astype('int')\n",
    "\n",
    "# appending class label\n",
    "feats_class_train = pd.merge(feats, scores_train[['spkID','genderclass','class']], how='inner')\n",
    "feats_class_test = pd.merge(feats, scores_test[['spkID','genderclass','class']], how='inner')\n",
    "\n",
    "print('Number of high instances in Train:', len(feats_class_train.loc[feats_class_train['class']=='high']) )\n",
    "print('Number of low instances in Train:', len(feats_class_train.loc[feats_class_train['class']=='low']) )\n",
    "print('Number of high instances in Test:', len(feats_class_test.loc[feats_class_test['class']=='high']) )\n",
    "print('Number of low instances in Test:', len(feats_class_test.loc[feats_class_test['class']=='low']) )\n",
    "\n",
    "feats_class_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardize speech features  \n",
    "\n",
    "# save feature names\n",
    "feats_names = list(feats_class_train.drop(['name','spkID','genderclass','class'],axis=1))\n",
    "\n",
    "myfile = open(r'.\\data_while_tuning\\feats_names.csv', 'w')\n",
    "for item in feats_names:\n",
    "    myfile.write(\"%r\\n\" % item)\n",
    "\n",
    "# learn transformation on training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(feats_class_train.drop(['name','spkID','genderclass','class'],axis=1))\n",
    "\n",
    "# numpy n_instances x n_feats\n",
    "feats_s_train = scaler.transform(feats_class_train.drop(['name','spkID','genderclass','class'],axis=1))\n",
    "feats_s_test = scaler.transform(feats_class_test.drop(['name','spkID','genderclass','class'],axis=1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model tuning with feature selection\n",
    "\n",
    "Use the train data to find the classifier and its hyperparameters leading to the best performance. \n",
    "\n",
    "Perform feature selection with \"SelectKBest\": selecting best k features based on ANOVA F-value computed between class label and feature. k ranging from 2 to total number of features in intervals of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Summarize results of cross-validation on set A for hyperparameter tuning\n",
    "\n",
    "Inputs:\n",
    "- cname: classifier name\n",
    "- grid_result: gridsearch results for this classifier, output of grid.fit(AX, Ay)  \n",
    "- filename: filename with path to write the summary to\n",
    "\"\"\"\n",
    "def summary_tuning(cname, grid_result, filename):\n",
    "    \n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "\n",
    "    # print best result and append to our lists\n",
    "    print(\"%r -> Best cross-val score on A set: %f using %s\" % (cname, grid_result.best_score_, grid_result.best_params_))\n",
    "    \n",
    "    # dataframe with summary    \n",
    "    d = {\n",
    "        'model': cname, \n",
    "        'mean_acc_A': means, \n",
    "        'stdev_acc_A': stds, \n",
    "        'params': params, \n",
    "    }\n",
    "    df = pd.DataFrame(data = d) \n",
    "    df.to_csv(filename, index=False)\n",
    "      \n",
    "                           \n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "Perform nested hyperparameter tuning.\n",
    "Given training data splitted into A, B sets and for each classifier type:\n",
    "Stratified cross-validation for feature selection and hyperparameter tuning using set A\n",
    "Generates csv file with summary of hp tuning (set A)\n",
    "Evaluate the performance on set B and return accs\n",
    "\n",
    "Input:\n",
    "- AX and BX: features of the train set, splitted\n",
    "- Ay and By: labels of the train set, splitted\n",
    "- get_cls_functions: list of functions tho get classifier and dict of hp to tune\n",
    "\n",
    "Output: \n",
    "- cls_acc_hps: pandas dataframe with:\n",
    "    - classifiers names\n",
    "    - classifiers hyperparameters\n",
    "    - selected features\n",
    "    - accuracies on B set\n",
    "    of each tuned classifier corresponding to get_cls_functions\n",
    "- trained_cls_list: list of classifiers tuned and trained on (AX+BX)\n",
    "\"\"\"    \n",
    "\n",
    "def hp_tuner(AX, BX, Ay, By, get_cls_functions):\n",
    "\n",
    "    # init lists (there will be one element per classifier in get_cls_functions)\n",
    "    \n",
    "    classifiers_names = []\n",
    "    classifiers = []\n",
    "    hparam_grids = []\n",
    "    best_accs = [] # on the B set\n",
    "    best_hps = [] # determined with CV on A\n",
    "    sel_feats_i = [] # indexes of selected features\n",
    "    sel_feats = [] # names of selected features\n",
    "    trained_cls_list = [] # tuned classifier trained on X,y\n",
    "    \n",
    "    # iterate over list of functions \n",
    "    # to get classifiers and parameters and append to our lists\n",
    "\n",
    "    for fn in get_cls_functions:     \n",
    "        clsname, cls, hp = fn()\n",
    "        classifiers_names.append(clsname)\n",
    "        classifiers.append(cls)\n",
    "        hparam_grids.append(hp)\n",
    "        \n",
    "    # tune hyperparameters with GridSearchCV for each classifier\n",
    "    \n",
    "    for i in np.arange(len(classifiers)):\n",
    "\n",
    "        # create pipeline\n",
    "        pipe = Pipeline([\n",
    "            ('selecter', SelectKBest(f_classif, k=4)),\n",
    "            ('classifier', classifiers[i])\n",
    "        ])\n",
    "        \n",
    "        # feature selection params\n",
    "        fsel_params = dict(\n",
    "            selecter__k = np.arange(2, AX.shape[1], 5)\n",
    "        )\n",
    "\n",
    "        # feature selection params and classifier's params for param_grid: \n",
    "        all_params = {**fsel_params, **hparam_grids[i]}\n",
    "        \n",
    "        # perform grid search\n",
    "        grid = GridSearchCV(estimator=pipe,\n",
    "                            param_grid=all_params,\n",
    "                            scoring='recall_macro', # average per-class accuracy \n",
    "                            n_jobs=1,\n",
    "                            cv=10)\n",
    "        \n",
    "        # This might take a while:\n",
    "        grid_result = grid.fit(AX, Ay) \n",
    "        \n",
    "        # summary of hp tuning on set A\n",
    "        # generate one csv file per classifier\n",
    "        summary_tuning(classifiers_names[i], \n",
    "                       grid_result, \n",
    "                       r'.\\data_while_tuning\\%s_tuning.csv' % classifiers_names[i])\n",
    "\n",
    "        # get selected features on set A\n",
    "        sel_i = grid_result.best_estimator_.named_steps['selecter'].get_support()\n",
    "        selected = [i for indx, i in enumerate(feats_names) if sel_i[indx]]\n",
    "        print(\"%r -> Selected features: %r\" % (classifiers_names[i], selected))\n",
    "        sel_feats_i.append(sel_i)\n",
    "        sel_feats.append(selected)\n",
    "        \n",
    "        # evaluate classifier on set B\n",
    "        By_pred = grid_result.best_estimator_.predict(BX)\n",
    "        score_on_B = recall_score(By, By_pred, average='macro')\n",
    "        print(\"%r -> Average per-class accuracy on B set: %f\\n\" % (classifiers_names[i], score_on_B))\n",
    "        \n",
    "        # add score on B and hyperparams for output\n",
    "        best_accs.append(score_on_B)\n",
    "        best_hps.append(grid_result.best_params_)\n",
    "        \n",
    "        # train classifier using all training data with this classifier\n",
    "        X = np.concatenate((AX, BX), axis=0)\n",
    "        y = np.concatenate((Ay, By), axis=0)\n",
    "        trained_cls = grid_result.best_estimator_.fit(X,y)\n",
    "        trained_cls_list.append(trained_cls)\n",
    "    \n",
    "    # create the output dataframe    \n",
    "    d = {\n",
    "        'classifiers_names': classifiers_names, \n",
    "        'best_accs': best_accs, \n",
    "        'best_hps': best_hps, \n",
    "        'sel_feats': sel_feats, \n",
    "        'sel_feats_i': sel_feats_i\n",
    "    }\n",
    "    cls_acc_hps = pd.DataFrame(data = d) \n",
    "    \n",
    "    return cls_acc_hps, trained_cls_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main code snippet to evaluate classification accuracy:\n",
    "    \n",
    "* Choose data (feature and labels) for train X and y and test Xt and yt\n",
    "* Split train data into A and B sets\n",
    "* Hyperparameter tuner using A and B sets data by calling hp_tuner()\n",
    "    * For each classifier type:\n",
    "        * Stratified cross-validation for hyperparameter tuning using set A\n",
    "        * Evaluate the performance on set B\n",
    "* Select classifier based on the best performance on set B and train it using all training data   \n",
    "* Get performance on test set\n",
    "\n",
    "(Nested hyperparameter tuning inspired by [A. Zheng](http://www.oreilly.com/data/free/evaluating-machine-learning-models.csp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training data. Features and labels\n",
    "X = feats_s_train\n",
    "y = feats_class_train['class'].cat.codes\n",
    "\n",
    "# test data. Features and labels\n",
    "Xt = feats_s_test\n",
    "yt = feats_class_test['class'].cat.codes\n",
    "\n",
    "# split train data into 80% and 20% subsets - with balance in trait and gender\n",
    "# give subset A to the inner hyperparameter tuner\n",
    "# and hold out subset B for meta-evaluation\n",
    "AX, BX, Ay, By = train_test_split(X, y, test_size=0.20, stratify = feats_class_train['genderclass'], random_state=2302)\n",
    "\n",
    "print('Number of instances in A (hyperparameter tuning):',AX.shape[0])\n",
    "print('Number of instances in B (meta-evaluation):',BX.shape[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataframe with results from hp tuner to be appended\n",
    "tuning_all = pd.DataFrame()\n",
    "\n",
    "# list with tuned classifiers trained on training data, to be appended\n",
    "trained_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save splits\n",
    "\n",
    "import csv\n",
    "\n",
    "# original features and class\n",
    "feats_class_train.to_csv(r'.\\data_while_tuning\\feats_class_train.csv', index=False)\n",
    "feats_class_test.to_csv(r'.\\data_while_tuning\\feats_class_test.csv', index=False)\n",
    "\n",
    "# train/test partitions, features and labels\n",
    "np.save(r'.\\data_while_tuning\\X.npy', X)\n",
    "np.save(r'.\\data_while_tuning\\y.npy', y)\n",
    "np.save(r'.\\data_while_tuning\\Xt.npy', Xt)\n",
    "np.save(r'.\\data_while_tuning\\yt.npy', yt)\n",
    "\n",
    "# # A/B splits, features and labels\n",
    "np.save(r'.\\data_while_tuning\\AX.npy', AX)\n",
    "np.save(r'.\\data_while_tuning\\BX.npy', BX)\n",
    "np.save(r'.\\data_while_tuning\\Ay.npy', Ay)\n",
    "np.save(r'.\\data_while_tuning\\By.npy', By)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Saving outpus of hp tuning to disk\n",
    "Called after tuning each classifier\n",
    "\n",
    "Input:\n",
    "- tuning_all: pandas df with tuning results\n",
    "- trained_all: list of all classifiers trained on training data\n",
    "\"\"\" \n",
    "def save_tuning(tuning_all, trained_all):\n",
    "    \n",
    "    # save tuning_all\n",
    "    tuning_all.to_csv(r'.\\data_while_tuning\\tuning_all.csv', index=False)\n",
    "    \n",
    "    # save trained_all\n",
    "    for i in np.arange(len(trained_all)):\n",
    "        filename = r'.\\data_while_tuning\\trained_' + tuning_all.loc[i, 'classifiers_names'] + '.sav'\n",
    "        pickle.dump(trained_all[i], open(filename, 'wb'))\n",
    "        \n",
    "\"\"\"\n",
    "Loading outpus of hp tuning from disk\n",
    "Called to recover what was tuned and trained in previous sessions\n",
    "\n",
    "Output:\n",
    "- tuning_all: pandas df with tuning results\n",
    "- trained_all: list of all classifiers trained on training data\n",
    "\"\"\" \n",
    "def load_tuning():\n",
    "    \n",
    "    # load tuning_all\n",
    "    tuning_all = pd.read_csv(r'.\\data_while_tuning\\tuning_all.csv')\n",
    "    \n",
    "    # load trained_all\n",
    "    trained_all=[]\n",
    "    for i in np.arange(len(tuning_all)):\n",
    "        filename = r'.\\data_while_tuning\\trained_' + tuning_all.loc[i, 'classifiers_names'] + '.sav'\n",
    "        loaded_model = pickle.load(open(filename, 'rb'))\n",
    "        trained_all.append(loaded_model)\n",
    "        \n",
    "    return tuning_all, trained_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling hp_tuner() for each classifier individually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Recover ** when new ipynb session started\n",
    "(Workaround for working with hyperparameter tuning during several days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# original features and class\n",
    "feats_class_train = pd.read_csv(r'.\\data_while_tuning\\feats_class_train.csv')\n",
    "feats_class_test = pd.read_csv(r'.\\data_while_tuning\\feats_class_test.csv')\n",
    "feats_names = pd.read_csv(r'.\\data_while_tuning\\feats_names.csv')\n",
    "\n",
    "# train/test partitions, features and labels\n",
    "X = np.load(r'.\\data_while_tuning\\X.npy')\n",
    "y = np.load(r'.\\data_while_tuning\\y.npy')\n",
    "Xt = np.load(r'.\\data_while_tuning\\Xt.npy')\n",
    "yt = np.load(r'.\\data_while_tuning\\yt.npy')\n",
    "\n",
    "# A/B splits, features and labels\n",
    "AX = np.load(r'.\\data_while_tuning\\AX.npy')\n",
    "BX = np.load(r'.\\data_while_tuning\\BX.npy')\n",
    "Ay = np.load(r'.\\data_while_tuning\\Ay.npy')\n",
    "By = np.load(r'.\\data_while_tuning\\By.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading outpus of hp tuning from disk\n",
    "tuning_all, trained_all = load_tuning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call this after each experiment: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save tuning_all (.csv) and trained_all (nameclassifier.sav)\n",
    "save_tuning(tuning_all, trained_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GaussianNB\n",
    "\n",
    "*class sklearn.naive_bayes.GaussianNB(priors=None)*\n",
    "\n",
    "No parameters to tune for this classifier. Priors not specified, so they will be adjusted given the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\"\"\"\n",
    "Naive Bayes Classifier\n",
    "\"\"\"\n",
    "def get_GaussianNB2tune():\n",
    "\n",
    "    model = GaussianNB()\n",
    "    hp = dict()\n",
    "    return 'GaussianNB', model, hp\n",
    "\n",
    "# Hyperparameter tuning with this model\n",
    "tuning, trained = hp_tuner(AX, BX, Ay, By, [get_GaussianNB2tune])\n",
    "\n",
    "# update lists of tuning info and trained classifiers\n",
    "tuning_all = tuning_all.append(tuning, ignore_index=True)\n",
    "trained_all.append(trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# open generated file with results of fitting GridSearchCV\n",
    " \n",
    "sgrid = pd.read_csv(r'.\\data_while_tuning\\GaussianNB_tuning.csv')\n",
    "print(sgrid['params'].head())\n",
    "\n",
    "# params to dataframe\n",
    "params_dict = sgrid['params'].apply(lambda x: literal_eval(x) ).to_dict()\n",
    "params_df = pd.DataFrame(data = params_dict).transpose()\n",
    "\n",
    "# plot acc vs. k\n",
    "sns.pointplot(x='selecter__k', y='mean_acc_A', data=sgrid.join(params_df)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not so good performance with naive bayes, and no trend can be seen to detect which number of selected features is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogisticRegression\n",
    "\n",
    "*class sklearn.linear_model.LogisticRegression(penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=’liblinear’, max_iter=100, multi_class=’ovr’, verbose=0, warm_start=False, n_jobs=1)*\n",
    "\n",
    "Tuning C (inverse of regularization strength). The 'liblinear' solver (a good choice for small datasets) handles L1 penalty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\"\"\"\n",
    "Logistic Regression\n",
    "\"\"\"\n",
    "def get_LogisticRegression2tune():\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    hp = dict(\n",
    "        #classifier__penalty = ['l1','l2'],\n",
    "        classifier__C = np.logspace(-3,3,num=7)\n",
    "    )\n",
    "    return 'LogisticRegression', model, hp\n",
    "\n",
    "# Hyperparameter tuning with this model\n",
    "tuning, trained = hp_tuner(AX, BX, Ay, By, [get_LogisticRegression2tune])\n",
    "\n",
    "# update lists of tuning info and trained classifiers\n",
    "tuning_all = tuning_all.append(tuning, ignore_index=True)\n",
    "trained_all.append(trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open generated file with results of fitting GridSearchCV\n",
    " \n",
    "sgrid = pd.read_csv(r'.\\data_while_tuning\\LogisticRegression_tuning.csv')\n",
    "\n",
    "# params to dataframe\n",
    "params_dict = sgrid['params'].apply(lambda x: literal_eval(x) ).to_dict()\n",
    "params_df = pd.DataFrame(data = params_dict).transpose()\n",
    "\n",
    "# plot acc vs. params\n",
    "sns.pointplot(x='selecter__k', y='mean_acc_A',hue='classifier__C', data=sgrid.join(params_df)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including more features is beneficial for the performance of logistic regression. Similar behavior when C >= 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\"\"\"\n",
    "K Nearest Neighbors\n",
    "\"\"\"\n",
    "def get_KNeighborsClassifier2tune():\n",
    "\n",
    "    model = KNeighborsClassifier()\n",
    "    hp = dict(\n",
    "        classifier__n_neighbors = list(range(1,40))\n",
    "    )\n",
    "    return 'KNeighborsClassifier', model, hp\n",
    "\n",
    "# Hyperparameter tuning with this model\n",
    "tuning, trained = hp_tuner(AX, BX, Ay, By, [get_KNeighborsClassifier2tune])\n",
    "\n",
    "# update lists of tuning info and trained classifiers\n",
    "tuning_all = tuning_all.append(tuning, ignore_index=True)\n",
    "trained_all.append(trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open generated file with results of fitting GridSearchCV\n",
    " \n",
    "sgrid = pd.read_csv(r'.\\data_while_tuning\\KNeighborsClassifier_tuning.csv')\n",
    "\n",
    "# params to dataframe\n",
    "params_dict = sgrid['params'].apply(lambda x: literal_eval(x) ).to_dict()\n",
    "params_df = pd.DataFrame(data = params_dict).transpose()\n",
    "\n",
    "# plot acc vs. params\n",
    "params_df = params_df.loc[params_df['classifier__n_neighbors']<10,:] # selecting only lower k\n",
    "sns.pointplot(x='selecter__k', y='mean_acc_A',hue='classifier__n_neighbors', data=sgrid.join(params_df)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of KNN classifiction tends to be better with more features with 6 neighbors or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\"\"\"\n",
    "Support Vector Machines\n",
    "\"\"\"\n",
    "def get_SVC2tune():\n",
    "    \n",
    "    model = SVC()\n",
    "    hp = dict(\n",
    "        classifier__C = np.logspace(-5,3,num=9),\n",
    "        classifier__kernel = ['poly'], #['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        classifier__degree = [2] #, # only 'poly' kernel\n",
    "        #classifier__gamma = np.logspace(-5,3,num=9)\n",
    "    )\n",
    "    return 'SVC', model, hp\n",
    "\n",
    "# Hyperparameter tuning with this model\n",
    "tuning, trained = hp_tuner(AX, BX, Ay, By, [get_SVC2tune])\n",
    "\n",
    "# # update lists of tuning info and trained classifiers\n",
    "# tuning_all = tuning_all.append(tuning, ignore_index=True)\n",
    "# trained_all.append(trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\"\"\"\n",
    "Decision Trees\n",
    "\"\"\"\n",
    "def get_DecisionTreeClassifier2tune():\n",
    "    \n",
    "    model = DecisionTreeClassifier()\n",
    "    hp = dict(\n",
    "        classifier__max_depth = np.arange(2,4)#np.arange(2,11)\n",
    "    )\n",
    "    return 'DecisionTreeClassifier', model, hp\n",
    "\n",
    "# Hyperparameter tuning with this model\n",
    "tuning, trained = hp_tuner(AX, BX, Ay, By, [get_DecisionTreeClassifier2tune])\n",
    "\n",
    "# # update lists of tuning info and trained classifiers\n",
    "# tuning_all = tuning_all.append(tuning, ignore_index=True)\n",
    "# trained_all.append(trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\"\"\"\n",
    "Random Forest\n",
    "\"\"\"\n",
    "def get_RandomForestClassifier2tune():\n",
    "    \n",
    "    model = RandomForestClassifier()\n",
    "    hp = dict(\n",
    "        classifier__n_estimators = np.arange(2,4)#np.arange(2,51)\n",
    "    )\n",
    "    return 'RandomForestClassifier', model, hp\n",
    "\n",
    "# Hyperparameter tuning with this model\n",
    "tuning, trained = hp_tuner(AX, BX, Ay, By, [get_RandomForestClassifier2tune])\n",
    "\n",
    "# # update lists of tuning info and trained classifiers\n",
    "# tuning_all = tuning_all.append(tuning, ignore_index=True)\n",
    "# trained_all.append(trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select the classifier that gave the maximum acc on B set\n",
    "best_accs = tuning_all['best_accs']\n",
    "i_best = best_accs.idxmax()\n",
    "\n",
    "print('Selected classifier based on the best performance on B: %r (accB = %0.2f)' % (tuning_all.loc[i_best,'classifiers_names'], round(best_accs[i_best],2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predictions on the test set \n",
    "# feat sel on Xt is performed by the classifier\n",
    "yt_pred = trained_all[i_best][0].predict(Xt)\n",
    "\n",
    "score_on_test = recall_score(yt, yt_pred, average='macro')\n",
    "\n",
    "print(\"Average per-class accuracy on test: %f\" % score_on_test) \n",
    "\n",
    "cm = confusion_matrix(yt, yt_pred)\n",
    "print(classification_report(yt, yt_pred, digits = 2))\n",
    "print(cm)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
